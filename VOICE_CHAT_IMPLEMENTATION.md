# Voice Chat Implementation Guide

This document describes the voice chat implementation using OpenAI's Realtime API in your Electron app.

## Overview

Voice chat has been fully integrated into your application, allowing users to have real-time voice conversations with the AI assistant. The implementation uses your backend's ephemeral token system to securely connect directly to OpenAI's Realtime API.

## Architecture

### Components Created

1. **`src/lib/voice-session-service.ts`**
   - Service for requesting ephemeral tokens from your backend
   - Handles `/api/v1/voice/session` endpoint communication
   - Includes proper error handling and timeout management

2. **`src/lib/openai-realtime-manager.ts`**
   - WebSocket manager for OpenAI Realtime API
   - Handles audio capture from microphone
   - Processes audio data (Float32 ‚Üí PCM16 conversion)
   - Manages WebSocket connection lifecycle
   - Handles incoming AI audio responses
   - Includes automatic reconnection logic

3. **`src/hooks/use-voice-chat.ts`**
   - React hook for managing voice chat state
   - Provides functions: `startVoiceChat()`, `stopVoiceChat()`, `toggleMute()`
   - Manages audio playback queue
   - Handles connection state updates

4. **`src/components/voice-chat.tsx`**
   - UI component for voice chat interface
   - Shows connection status with visual indicators
   - Displays real-time transcripts
   - Provides mute/unmute and stop controls
   - Includes helpful tips for users

5. **Integration in `src/hooks/use-app-logic.ts`**
   - Added voice chat state management
   - Integrated with existing navigation logic
   - Ensures only one panel is open at a time

6. **Integration in `src/App.tsx`**
   - Renders VoiceChat component when active
   - Properly passes state and handlers

7. **Updated `src/lib/api-client.ts`**
   - Added voice session types and interfaces
   - Added `createVoiceSession()` method to ApiClient

## How It Works

### 1. Starting Voice Chat

When a user clicks the "Speak" button in the navigation bar:

1. The app calls your backend's `/api/v1/voice/session` endpoint with:

   ```json
   {
     "game": "game name" // or null
   }
   ```

2. Your backend responds with an ephemeral token:

   ```json
   {
     "client_secret": "eph_sk_...",
     "ephemeral_key_id": "eph_key_...",
     "model": "gpt-realtime-mini-2025-10-06",
     "expires_at": 1234567890,
     "websocket_url": "wss://api.openai.com/v1/realtime?model=...",
     "connection_instructions": {...}
   }
   ```

3. The app creates a WebSocket connection to OpenAI with:
   - Authorization header: `Bearer ${client_secret}`
   - OpenAI-Beta header: `realtime=v1`

4. Once connected, the app:
   - Configures the session (voice, audio formats, turn detection)
   - Starts capturing audio from the user's microphone
   - Converts audio to PCM16 format
   - Sends audio chunks to OpenAI via WebSocket

### 2. Audio Processing

**Outgoing Audio (User ‚Üí OpenAI):**

- Captures audio using Web Audio API (MediaStream)
- Processes through AudioWorklet (runs on audio rendering thread)
- Converts Float32Array ‚Üí Int16Array (PCM16) in the worklet
- Base64 encodes and sends via WebSocket from main thread
- Format: 24kHz, mono, PCM16

**Incoming Audio (OpenAI ‚Üí User):**

- Receives base64-encoded PCM16 chunks
- Adds to playback queue
- Converts PCM16 ‚Üí AudioBuffer
- Plays through Web Audio API
- Handles queue automatically

### 3. Features

- **Voice Activity Detection (VAD)**: Server-side turn detection with configurable thresholds
- **Real-time Transcription**: User's speech is transcribed using Whisper
- **Interruption Detection**: Automatically detects when you speak and cancels AI's response
- **Mute/Unmute**: Control microphone input
- **Connection Status**: Visual indicators for connecting/connected/error states
- **Error Handling**: Graceful error messages and automatic reconnection (up to 3 attempts)
- **Audio Queue**: Smooth playback of AI responses without gaps or overlaps

### 4. Interruption Detection

The voice chat supports **natural interruptions** - you can speak while the AI is talking, and it will automatically stop and listen to you.

**How it works:**

1. **Audio Level Monitoring**: The AudioWorklet continuously calculates the RMS (Root Mean Square) audio level of your microphone input
2. **AI Speaking Detection**: Tracks when the AI is currently generating audio responses
3. **Interruption Detection**: When audio level exceeds the threshold while AI is speaking, sends `response.cancel` to OpenAI
4. **Seamless Transition**: AI stops talking, and your speech is processed normally

**Configuration:**

```typescript
// Adjust interruption sensitivity
voiceChat.setInterruptionConfig({
  enabled: true, // Enable/disable interruptions (default: true)
  threshold: 0.01, // Audio level threshold 0.0-1.0 (default: 0.01)
})
```

**Sensitivity Guide:**

- `0.005` - Very sensitive (may trigger on background noise)
- `0.01` - Default (good for quiet environments)
- `0.02` - Less sensitive (better for noisy environments)
- `0.05` - Very low sensitivity (requires loud/clear speech)

**Audio Level (RMS):**

- Silence: ~0.0
- Quiet speech: 0.01-0.05
- Normal speech: 0.05-0.2
- Loud speech: 0.2+

## Configuration

### Session Configuration

**Important**: Voice and instructions are configured by the backend when creating the ephemeral token. The backend automatically tailors the AI's personality and instructions based on the detected game.

The client only configures these technical settings in `openai-realtime-manager.ts`:

```typescript
{
  modalities: ['text', 'audio'],
  input_audio_format: 'pcm16',
  output_audio_format: 'pcm16',
  input_audio_transcription: {
    model: 'whisper-1'
  },
  turn_detection: {
    type: 'server_vad',
    threshold: 0.5,
    prefix_padding_ms: 300,
    silence_duration_ms: 500
  }
}
```

**What's configured where:**

- **Backend (via ephemeral token)**: Voice, instructions, model
- **Client (via session.update)**: Audio formats, modalities, turn detection

This ensures the AI is properly tailored to each game without the client needing to know game-specific instructions.

## User Experience

### Voice Chat UI

When voice chat is active, users see:

1. **Status Indicator**:
   - üîÑ Spinner: Connecting
   - üé§ Green mic with pulse: Listening
   - üîá Gray mic: Muted
   - ‚ö†Ô∏è Red alert: Error

2. **Transcript Display**: Shows what the user said (from Whisper transcription)

3. **Controls**:
   - Microphone button: Mute/unmute
   - Phone button (red): Stop voice chat

4. **Tips**: Helpful guidance for users

### Integration with Existing Features

- Voice chat automatically closes other panels (text chat, settings, history)
- Only one communication method is active at a time
- Properly integrated with keyboard shortcuts
- Respects transparency settings
- Uses interactive areas for click-through overlay

## Security Considerations

‚úÖ **Secure Implementation:**

- Ephemeral tokens are short-lived (60 seconds)
- Tokens are obtained per-session from your authenticated backend
- No API keys exposed in frontend code
- Uses your existing Auth0 JWT authentication
- Direct WebSocket connection to OpenAI (no sensitive data through your backend)

## Browser Compatibility

**Requirements:**

- Modern browser with Web Audio API support
- Microphone access permission
- WebSocket support

**Tested:**

- Chrome/Edge (Electron uses Chromium)
- Should work in all Electron apps

## Performance

- **Audio Processing**: AudioWorklet (runs on separate audio thread for low latency)
- **Audio Buffer Size**: 128 samples (AudioWorklet default, minimal latency)
- **Sample Rate**: 24kHz (optimal for voice)
- **Audio Format**: PCM16 (efficient)
- **Network**: Low bandwidth usage due to audio compression
- **Thread Safety**: Audio processing runs on dedicated audio rendering thread

## Troubleshooting

### Common Issues:

1. **"Failed to start audio capture"**
   - Ensure microphone permissions are granted
   - Check if another app is using the microphone

2. **"Connection timeout"**
   - Check internet connection
   - Verify backend is accessible
   - Ephemeral token might have expired

3. **"Authentication failed"**
   - User needs to re-authenticate
   - Auth0 token might be expired

4. **No audio output**
   - Check browser audio settings
   - Verify audio output device is working

## Future Enhancements

Potential improvements you could add:

1. **Voice Selection**: Allow users to choose between different AI voices
2. **Audio Visualizer**: Show waveform of user's speech
3. **Conversation History**: Save voice conversation transcripts
4. **Push-to-Talk**: Alternative to always-on listening
5. **Noise Cancellation**: Advanced audio processing before sending (e.g., noise gate, filters)
6. **Language Selection**: Support multiple languages for transcription
7. **Volume Controls**: Adjust AI voice volume
8. **Voice Activity Visualization**: Show when AI detects voice activity

## API Documentation

For the most up-to-date information about OpenAI's Realtime API, refer to:

- [OpenAI Realtime API Documentation](https://platform.openai.com/docs/api-reference/realtime)
- [WebSocket Events Reference](https://platform.openai.com/docs/api-reference/realtime-events)

## Testing Checklist

- [ ] Click "Speak" button to start voice chat
- [ ] Verify microphone permission prompt appears
- [ ] Check connection status changes: connecting ‚Üí connected
- [ ] Speak and verify transcript appears
- [ ] Verify AI responds with voice
- [ ] Test mute/unmute functionality
- [ ] Test stop button
- [ ] Verify voice chat closes when opening other panels
- [ ] Test error handling (disconnect internet, etc.)
- [ ] Check that selected game is passed to backend

## Code Quality

‚úÖ All code:

- Follows TypeScript best practices
- Includes comprehensive error handling
- Has proper type definitions
- Uses async/await correctly
- Includes cleanup in React useEffect hooks
- Follows your existing code style and patterns
- Passes ESLint with no errors

---

**Implementation Complete!** Voice chat is now fully integrated and ready to use. The "Speak" button in your navigation bar will activate the voice chat feature.
